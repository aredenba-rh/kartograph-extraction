{"user_input": "How do I login to a cluster using backplane?", "reference": "Run this command: `ocm backplane login <cluster_id>`"}
{"user_input": "How can I get the infrastructure id for a cluster?", "reference": "To get the infrastructure id from inside the cluster use:\n`oc get infrastructures cluster -o jsonpath='{.status.infrastructureName}{\"\n\"}'`\nor from outside the cluster:\n`ocm get cluster <interClusterID> | jq -r .infra_id`"}
{"user_input": "How do I get the Dynatrace tenant URL for a cluster I'm working on without using osdctl?", "reference": "There are two ways to get the Dynatrace tenant URL without using osdctl.\nMethod 1: Using OCM\n\n`echo $(ocm get $(ocm get cluster ${INTERNAL_ID} | jq -r .subscription.href)/labels | jq -r '.items[] | select(.key == \"dynatrace.regional-tenant\") | .value')'.apps.dynatrace.com/ui'\n\nMethod 2: Getting the information from the cluster itself\n\n`oc -n dynatrace get dynakube -ojson | jq -r '.items[].spec.apiUrl' | sed 's/live.dynatrace.com\/api/apps.dynatrace.com\/ui/'`"}
{"user_input": "How do I login to a management cluster?", "reference": "You login to a management cluster the same way you would login to a customer cluster, use:\n`ocm backplane login $MC_CLUSTER_ID`"}
{"user_input": "I am investigating an issue with a cluster, and I need to login to its management cluster. How do I login to the management cluster for this cluster I am investigating?", "reference": "You can login to a management cluster of a cluster in question using:\n`ocm backplane login ${CLUSTER_ID} --manager`"}
{"user_input": "How do I post a servicelog to a cluster?", "reference": "To post a servicelog to a cluster:\n\n**Single cluster:**\n`osdctl servicelog post --cluster-id ${CLUSTER_ID} -t <template-file-or-url> -p JIRA_ID=<JIRA_Ticket>`\n\n**Multiple clusters by query:**\n`osdctl servicelog post -q \"managed='true' and region.id like '%us-east-1%'\" -t <template>`\n\n**Multiple clusters from file:**\n`osdctl servicelog post -c clusters.json -t template.json`\n\n**Common flags:**\n- `--cluster-id` or `-C` - Cluster UUID or name\n- `--template` or `-t` - Template file or URL\n- `--param` or `-p` - Set parameters (format: `-p KEY=VALUE`)\n- `--dry-run` - Preview without sending\n- `--internal` - Internal-only message\n- `--yes` - Skip confirmation prompts"}
{"user_input": "Using the `rosa` CLI, how do I create the account roles for a HCP cluster?", "reference": "**Recommended (automatic creation):**\n`rosa create account-roles --hosted-cp --mode auto`\n\n**Alternative (manual review first):**\n`rosa create account-roles --hosted-cp --mode manual`\n\nThe `--hosted-cp` flag is required for HCP clusters (creates different roles than Classic ROSA). Auto mode is recommended for most cases. Manual mode generates AWS CLI commands for you to review and run yourself."}
{"user_input": "I would like to upgrade the control plane on Red Hat Openshift on AWS cluster that is using Hosted Control Planes. I already know that I wish to upgrade to version 4.15.2. How do I perform this upgrade?", "reference": "For ROSA HCP clusters, upgrade the control plane using the ROSA CLI:\n\n**Immediate upgrade:**\n`rosa upgrade cluster --cluster <cluster_name_or_id> --version 4.15.2 --control-plane`\n\n**Scheduled upgrade:**\n`rosa upgrade cluster --cluster <cluster_name_or_id> --version 4.15.2 --schedule-date \"2025-10-21\" --schedule-time \"02:00\" --control-plane`\n\n**Important for HCP:** Control plane and machine pools upgrade separately. After control plane upgrade completes, separately upgrade machine pools:\n`rosa upgrade machinepool --cluster <cluster-name> --version 4.15.2 <machinepool-name>`\n\nList machine pools with: `rosa list machinepools --cluster <cluster-name>`"}
{"user_input": "On what software is the Red Hat Openshift GitOps system based?", "reference": "Red Hat OpenShift GitOps is based on Argo CD"}
{"user_input": "I'm an administrator of an Openshift ROSA cluster. I wish to add one of my coworker as another administrator. Should I add them as a `dedicated-admin` or a `cluster-admin`?", "reference": "The `dedicated-admin` role is the recommended role for users requiring standard access to the cluster. Only use the `cluster-admin` role if your coworker requires complete unrestricted access to all cluster resources, including those managed by Red Hat SRE."}
{"user_input": "When debugging a ROSA HCP cluster, what namespace do I find the control plane pods in on its management cluster?", "reference": "You can find the pods for a cluster on its management cluster inside the following nanmespace:\n`ocm-<env>-<cluster-id>-<cluster-name>`\n\nWhere:\n- `<env>` is the environment that the cluster is in\n- `<cluster-id>` is the clusters internal id\n- `<cluster-name>` is the clusters name"}
{"user_input": "I'm debugging a HCP cluster, and I need to view the HostedCluster object. Where do I find it?", "reference": "The HostedCluster object can be found on the management cluster, in a namespace named after the cluster it contains. It will be named as follows:\n`ocm-<env>-<cluster_id>`\n\nWhere:\n- `<env>` is the environment the cluster is in\n- `<cluster_id>` is the internal id of the cluster"}
{"user_input": "In a ROSA Classic cluster, which namespace would I find the `etcd` pods?", "reference": "The etcd pods are located in the openshift-etcd namespace."}
{"user_input": "As an SRE, I was just trying to delete a pod in a customer namespace, I know that this should be possible as it was mentioned in a SOP. How can I delete this pod with elevated permissions, what command do I run?", "reference": "There are two methods for elevating SRE privileges on a customer cluster. The preferred method is:\n`ocm-backplane elevate \"<REASON>\" -- delete pod <POD_NAME> -n <NAMESPACE>`\nor Method 2:\n`oc --as backplane-cluster-admin delete pod <POD_NAME> -n <NAMESPACE>`\nNote: Using a Method 2 will result in a compliance ticket being created."}
{"user_input": "I've got an alert regarding UpgradeNodeDrainFailedSRE, what could be blocking the node drain?", "reference": "Common causes blocking node drain:\n\n1. **Pod Disruption Budgets (PDBs)** - Check with `oc get pdb -A`\n2. **Pods not draining** - Customer workloads stuck or unresponsive\n3. **Customer webhooks** - Webhooks preventing pod eviction\n\n**Known Issue - MC Clusters:**\nIf this is a Management Cluster worker node upgrade, etcd in CrashLoopBackOff may block drain. See v4/alerts/hypershift/MachineOutOfCompliance.md#node-replacement-blocked-by-pdb for workaround. No incident needed for this issue.\n\n**Investigation Steps:**\n1. Check if HyperShift infrastructure cluster\n2. Get node name from alert\n3. Login to cluster\n4. Check machine logs: `oc describe machine <machine-name> -n openshift-machine-api`\n5. Start must-gather if needed"}
{"user_input": "How do I change cluster ownership?", "reference": "The process has been automated using the `osdctl` command:\n\n```sh\nosdctl cluster transfer-owner \\\n  --cluster-id $CLUSTER_ID \\\n  --new-owner \"$NEW_USER_ID\" \\\n  --reason \"$REASON\"\n```"}
{"user_input": "What is the difference between ROSA and ARO?", "reference": "ROSA (Red Hat OpenShift Service on AWS) is a fully-managed service that runs the Red Hat OpenShift enterprise Kubernetes platform on Amazon Web Services (AWS), while ARO (Azure Red Hat OpenShift) is a cluster that runs on Microsoft Azure."}
{"user_input": "How does billing work for ROSA, who pays for what?", "reference": "Red Hat OpenShift Service on AWS classic architecture is billed directly to your Amazon Web Services (AWS) account. ROSA pricing is consumption based, with annual commitments or three-year commitments for greater discounting."}
{"user_input": "How do I know who is responsible for managing control plane in ROSA and what my responsibilities as a customer?", "reference": "This is a combination of multiple parties involved. While Red Hat and Amazon Web Services (AWS) manage the Red Hat OpenShift Service on AWS classic architecture services, the customer shares certain responsibilities. The Red Hat OpenShift Service on AWS classic architecture services are accessed remotely, hosted on public cloud resources, created in customer-owned AWS accounts, and have underlying platform and data security that is owned by Red Hat."}
{"user_input": "I am performing testing and want to use AWS Spot instances on to save on cost. How can I use AWS Spot instances on ROSA?", "reference": "It is indeed cost-efficient to use AWS Spot instances to save on cost and take advantage of unused EC2 capacity. If you want to use it for ROSA then you need to create a machinepool with the following configuration:\n\n```json\n{\n  \"id\": \"spottest\",\n  \"instance_type\": \"m5.xlarge\",\n  \"replicas\": 3,\n  \"aws\": {\n    \"use_spot_instances\": true,\n    \"spot_market_options\": {\n      \"max_price\": 0.0646\n    }\n  }\n}\n```\n\nSave this configuration to a file (e.g., `spot.json`) and invoke the API by running:\n`ocm post /api/clusters_mgmt/v1/clusters/<cluster_id>/machine_pools <<< $(cat spot.json)`"}
{"user_input": "I have a ROSA customer who is complaining that ROSA upgrade path (from 4.18.26 to 4.19.16) is blocked today, but it was open yesterday. How do I check it and open the path if it's not a ROSA issue?", "reference": "Check the upgrade path using https://access.redhat.com/labs/ocpupgradegraph/update_path. If the path shows in the OCP Graph but is blocked for ROSA, it's likely blocked by ClusterImageSets due to upgrade risks. Check the allowed risks at https://gitlab.cee.redhat.com/service/clusterimagesets/-/blob/master/config/prod.yaml under `allowed_conditional_edge_risks`. To unblock, discuss with @clusterimagesets maintainers or TLs first."}
{"user_input": "I've received a PageDuty alert \"UpgradeNodeDrainFailedSRE CRITICAL\" which was grouped with another alert \"UpgradeNodeUpgradeTimeoutSRE CRITICAL\" for the same cluster. Which one I need to check and what are my steps?", "reference": "**Check UpgradeNodeDrainFailedSRE first** - it's the root cause. The drain failure blocks the upgrade, which then triggers UpgradeNodeUpgradeTimeoutSRE as a consequence.\n\n**Steps:**\n1. Open the UpgradeNodeDrainFailedSRE alert for the cluster ID\n2. Check if it's a HyperShift MC cluster (known etcd CrashLoopBackOff issue)\n3. Get the node name from alert labels\n4. Login to cluster: `ocm backplane login <cluster-id>`\n5. If MC cluster, see v4/alerts/hypershift/MachineOutOfCompliance.md for workaround (no incident needed)\n6. Otherwise, check machine logs and status\n7. Start must-gather if needed\n\nFixing the drain issue resolves both alerts."}
{"user_input": "The customer has raised a concern that SREs resized their infra/master nodes without prior acknowledgement from their side. As far as I know, such actions do not typically require customer approval. However, if the customer prefers that Red Hat seeks explicit approval before performing such activities, what are the possible ways to do so?", "reference": "**You are correct** - SRE typically does NOT require customer approval for infra/master node resizing. This is standard practice to maintain cluster stability.\n\n**When Management Approval IS Required:**\n- Increasing control plane beyond m5.8xlarge (non-CCS clusters only)\n- Decreasing control plane when cluster has >30% utilization in past week\n\n**If Customer Wants Pre-Approval Process:**\n1. **Proactive Ticket** - SRE opens support case explaining resize need, waits for customer acknowledgment before proceeding (unless critical incident)\n2. **Support Case Process** - Update active case to request approval\n3. **Custom SLA** - Contact account team for formal process modification\n\n**Trade-offs:**\n- Standard: Service log sent AFTER resize (notification, not approval)\n- Pre-approval: May delay remediation and risk cluster stability\n\nRecommend discussing with account team for formalized pre-approval process."}
{"user_input": "I am investigating an alert and need to log into customer's AWS console to review CloudTrail logs. How do I get console access and what command do I need to execute?", "reference": "In order to get AWS cloud console access, an engineer must have access to backplane and be able to log into a cluster's AWS console with `ocm backplane cloud console $CLUSTER_ID`."}
{"user_input": "I am investigating alerts and need to triage warning alerts, how do I find alert definitions?", "reference": "To locate alert definitions, use the OpenShift cluster CLI connection and run: `oc get prometheusrules -A -o yaml`"}

{"user_input": "Customer is not able to transfer the cluster ownership, the button is disabled even though they've been the org administrator and have cluster admin rights, can SRE help with ownership transfer?", "reference": "Yes, SRE can help. The ownership transfer button is currently disabled in UI (JIRA XCMSTRAT-293) - all transfers temporarily require SRE manual intervention.\n\n**Command:**\n`osdctl cluster transfer-owner --cluster-id $CLUSTER_ID --new-owner \"$NEW_USER_ID\" --reason \"$REASON\"`\n\n**SRE manual intervention also required when:**\n- Current and new owner in different OCM organizations\n- Cluster version ≤ 4.10\n- OSD Non-CCS clusters (without cluster-admin privileges)\n\n**For HCP clusters:** Only manual process works currently (use osdctl command above).\n\nThis must be done by Region Leads. See https://access.redhat.com/solutions/6126691 for details."}

{"user_input": "Does Red Hat provide or manage a notification layer that alerts both the customer team and Red Hat simultaneously when such incidents occur?", "reference": "Yes, Red Hat provides a notification system that alerts both customers and Red Hat:\n\n**For Customers:**\n- Service Log system sends email notifications for incidents (upgrades, failures, action required)\n- Emails sent for unplanned events, ongoing incidents, and required customer actions\n\n**For Red Hat SRE:**\n- Every customer notification is BCC'd to sd-customer-notification@redhat.com for visibility\n- PagerDuty alerts for critical issues (P1-P5 priority system)\n- SRE paged for critical-severity alerts from openshift-monitoring namespace\n\nWhen incidents occur, customers receive emails and Red Hat SRE teams are automatically copied, ensuring both parties are informed simultaneously (though through different channels: email for customers, PagerDuty for SRE)."}
{"user_input": "How can I quickly create an OSD cluster on AWS and have it live for 96 hours?", "reference": "**For stage/testing environments only** (not available in production):\n\n1. Login to staging: `ocm login --url staging --token $STG_TOKEN`\n2. Create cluster:\n   - ROSA: `rosa create cluster -c <clustername> --profile <AWS profile>`\n   - OR OCM: `ocm create cluster <name> --provider aws --region us-east-1`\n3. Extend expiration: `ocm edit cluster <cluster-name> --expiration 96h`\n\nStage clusters default to 72 hours, can be extended up to 168 hours maximum. Production clusters do not support expiration settings."}
{"user_input": "I am investigating a cluster with etcd pods are crashing, how do I check the overall health of etcd pods using osdctl?", "reference": "In order to check status of etcd pods using osdctl run command `osdctl cluster etcd-health-check --cluster-id <cluster_id> --reason <reason>`"}
{"user_input": "I need to find a full list of all management clusters, what is the easiest way?", "reference": "There are multiple ways to get a list of all Management clusters but the quickest is to query an OCM endpoint using command: `ocm get /api/osd_fleet_mgmt/v1/management_clusters | jq -r '.items[] | .name + \"        \" + .parent.name + \"        \" + .sector + \"           \" + .region + \"      \" + .status'`"}
{"user_input": "I am investigating an alert for a ROSA cluster and need to understand what is loading the CPU for the node for 60min. What is the Prometheus query that provides such info for a particular node?", "reference": "Use these Prometheus queries in the cluster console (Observe → Metrics):\n\n**Query 1 - Node-level CPU percentage:**\n```promql\n100 * sum by (instance) (rate(node_cpu_seconds_total{mode!=\"idle\"}[5m])) / sum by (instance) (rate(node_cpu_seconds_total[5m]))\n```\nShows overall CPU usage % for the node over time.\n\n**Query 2 - Top CPU consuming containers/pods on specific node:**\n```promql\ntopk(10, sum by (container, pod, namespace, node) (rate(container_cpu_usage_seconds_total{node=\"<node-name>\"}[5m])))\n```\nReplace `<node-name>` with your node name. Shows which specific pods/containers are consuming CPU.\n\nBoth use 5m rate windows. Use Prometheus UI period selector to view last 1hr-24hr window for 60min analysis."}
{"user_input": "Where is the control plane for a HCP cluster, how to find all control plane pods?", "reference": "Based on HCP architecture the control plane for a cluster resides on Management Cluster. In order to list all pods for HCP cluster you need to perform a couple of steps: Log into the associated management cluster using `ocm backplane login ${CLUSTER_ID} --manager`, identify the respective HCP namespace using command `oc get ns|grep ${HOSTED_CLUSTER_ID}`, and then list all pods for the associated HCP namespace to get its status using command `oc get po -n ocm-${env}-${clusterid}-${clustername}`."}
{"user_input": "I want to test and whitelist monitoring metrics for HCP cluster, how can I do so?", "reference": "To test metrics before whitelisting: (1) `ocm backplane login ${MANAGEMENT_CLUSTER}`, (2) `oc port-forward -n openshift-observability-operator svc/hypershift-monitoring-stack-prometheus 9090`, then check metrics at localhost:9090. To whitelist: For HCP metrics, add to Monitoring Stack CR in OSD Fleet Manager. For MC metrics, add to hypershift-cmo-federate service monitor regex. For Data Plane, add to hypershift-dataplane-metrics-forwarder."}
{"user_input": "The ControlPlaneNodesNeedResizingSRE is firing, I need to trigger a control plane resize, what commands do I run?", "reference": "Patch the ControlPlaneMachineSet with new instance type:\n```bash\nINSTANCE_TYPE=<new_type>\nocm-backplane elevate -n -- -n openshift-machine-api patch controlplanemachineset cluster --type merge -p '{\"spec\":{\"template\":{\"machines_v1beta1_machine_openshift_io\":{\"spec\":{\"providerSpec\":{\"value\":{\"instanceType\":\"'$INSTANCE_TYPE'\"}}}}}}}}'\n```\nFor full procedure including verification steps, see v4/howto/resize-control-plane.md"}
{"user_input": "I've just collected mustgather for HCP and would like to analyze it, but can't find a way of doing so. What is the possible way to read must-gather that's been collected?", "reference": "There are two possible ways to analyze MG file. Method 1: Use OMC binary from https://github.com/gmeghnag/omc example: `omc use controlplane-hsmg-1` Method 2: Use OMG binary from https://github.com/kxr/o-must-gather example: `omg use controlplane-hsmg-1`"}
{"user_input": "I am investigating an HCP cluster issue and need to collect data for the future analysis, I know there is a way to collect using must-gather but don't remember how. Give me a command of how to collect MG for HCP?", "reference": "The HCP has a different approach collecting must-gather compare to Classic Rosa. In order to collect MG for HCP you need to run `osdctl hcp must-gather --cluster-id ${CLUSTER_ID} --reason ${REASON}`"}
{"user_input": "I am investigating an alert for HCP and identified that the request serving node requires autoscaling. I am looking for commands that I need to run to perform this activity?", "reference": "For each cluster the HostedControlPlane has 2 request serving nodes. Follow steps: 1. Login into a cluster with `ocm backplane login $CLUSTER_ID --manager` 2. Set the CLUSTER_ID with `CLUSTER_ID=$INTERNAL_CLUSTER_ID` 3. Set CLUSTER_DOMAIN_PREFIX with `CLUSTER_DOMAIN_PREFIX=$(oc get hostedclusters.hypershift.openshift.io -n ocm-production-${CLUSTER_ID} -l api.openshift.com/id=${CLUSTER_ID} -o json | jq -r '.items[0].metadata.name')` 4. Find the currently assigned request with `oc get hostedcluster -n ocm-production-${CLUSTER_ID} -oyaml | grep hypershift.openshift.io/hosted-cluster-size` 5. Identify available sizes with `oc get clustersizingconfigurations.scheduling.hypershift.openshift.io cluster -o jsonpath='{range .spec.sizes[*]}{@.name}{\" -> \"}{@.criteria}{\"\\n\"}{end}'` 6. Override with `ocm backplane elevate \"${REASON}\" -- annotate hostedcluster -n ocm-production-${CLUSTER_ID} ${CLUSTER_DOMAIN_PREFIX} hypershift.openshift.io/cluster-size-override=${SIZE}`"}
{"user_input": "I am troubleshooting a ROSA cluster and trying to get describe node and get pods running on the node with `ocm backplane elevate \"reason\" -- describe node/node_name` but I get error `Error from server (Forbidden): unknown`. It seems that customer has enabled Access Protection feature. How do I request access to the cluster?", "reference": "By enabling Access Protection feature is a way for the customer to prevent users using backplane. In order to gain access you need to follow steps: 1. Creating an access request with `ocm backplane accessrequest create` 2. Wait for the request to be approved, check access request status with `ocm backplane accessrequest get` 3. Expire access request once you are done with the investigation with `ocm backplane accessrequest expire`. There is another method of performing these steps and can also be done using OCM API calls"}
{"user_input": "I have completed my testing and would like to save cost by deleting my ROSA cluster. How can I delete my cluster?", "reference": "There are a couple of ways to delete a ROSA cluster: Method 1: Delete a cluster using ROSA CLI `rosa delete cluster --cluster=<cluster_name>` Method 2: Navigate to OCM https://console.redhat.com/openshift/, then Actions \"delete\""}
{"user_input": "I have deployed a ROSA cluster and want to login as cluster-admin but am not sure how to find the Console URL I need to copy into a browser. How can I find the Console URL?", "reference": "In order to find the Console URL you can use two different ways: Method 1: Use ROSA CLI and run command `rosa describe cluster -c <cluster_name> | grep Console` Method 2: Use oc binary and run command `oc whoami --show-console`"}
{"user_input": "As an SRE, I am troubleshooting a ROSA cluster and would like to verify if Zero Egress feature is enabled. What command can I use to verify if Zero Egress is enabled for this cluster?", "reference": "In order to verify if Zero Egress is enabled you can use different ways. Method 1: Use `ocm` binary and run command `ocm get cluster ${CLUSTER_ID} | jq .properties.zero_egress` - if it returns `true` then it is enabled, if `false` then it is disabled. Method 2: Use `rosa` binary and run command `rosa describe cluster --cluster <CLUSTER_NAME>` and look for zero egress."}
{"user_input": "I am investigating a networking issue and was suggested to check if cluster wide proxy is enabled. How can I check this?", "reference": "You can find out if cluster wide proxy is enabled or not with different ways. Method 1: Use OCM CLI and run `ocm list clusters --managed --parameter search=\"proxy.https_proxy != '' or proxy.http_proxy != ''\"` Method 2: Log into a cluster, use `oc` CLI binary and run `oc get proxy -o=jsonpath=\"{.items[*]['spec','status']}\" | jq .`"}
{"user_input": "I was trying to delete a cluster but can see it stuck in `Uninstalling` state. Is there a way I can unblock and get the cluster to complete deprovisioning?", "reference": "If a cluster is stuck in `uninstalling` or `error` state you can always attempt to best effort delete the cluster by running command `ocm delete cluster $INTERNAL_CLUSTER_ID -p best_effort=true`"}
{"user_input": "We have a team member who has left the company and we need to remove his access to the cluster. He used to be a cluster-admin, how can we revoke his cluster-admin permissions?", "reference": "If you want to revoke existing cluster-admin permissions run the command `rosa revoke user cluster-admin --user=<idp_user_name> --cluster=<cluster_name>` and verify if it's been removed by running `rosa list users --cluster=<cluster_name>`"}
{"user_input": "As an SRE I am investigating a cluster issue with GCP Permissions for OSD cluster. I need to find out the project ID in order to identify GCP console. What command can I run?", "reference": "To get the GCP project ID for an OSD cluster, use this command within the cluster:\n\n**Primary method:**\n`oc get projectClaim -n uhc-production-<CLUSTER_ID>`\n\n**Alternative from cluster infrastructure:**\n`oc get infrastructure cluster -o jsonpath='{.status.platformStatus.gcp.projectID}'`\n\nReplace `<CLUSTER_ID>` with your cluster's internal ID. You can get the cluster ID from OCM or the cluster name."}
{"user_input": "After the initial investigation it was determined that the cluster is out of sync with Hive, I need to force the cluster's SyncSets sync. How can this be achieved?", "reference": "In order to force all of a cluster's SyncSets as well as SelectorSyncSets to be resynced, delete the cluster's ClusterSync object with the following command `osdctl cluster resync --cluster-id \"${CLUSTER_ID}\"` which will force Hive to immediately resync all applicable resources"}
{"user_input": "I would like to deploy an operator from the operator hub but not sure if it will be fully supported by Red Hat. This operator is marked as `community operator`, how do I find out if it's fully supported by SREs?", "reference": "Red Hat workloads typically refer to Red Hat-provided Operators that are fully supported. Any workload that is not managed by the Red Hat SRE team is considered `customer workload` and must be deployed on worker nodes. If the operator is marked as `community operator` then it's not supported by SREs and is considered customer workload."}
{"user_input": "I am planning to deploy a ROSA Classic cluster and need to make sure that it gets deployed without any issues but I have a requirement to deploy a firewall. What needs to be done to make sure the cluster gets deployed and supported?", "reference": "If you are planning to deploy a ROSA cluster whether it's Classic or HCP you need to meet minimum firewall requirements. Whitelist the following URLs: `pull.q1w2.quay.rhcloud.com`, `catalog.redhat.com`, `oidc.op1.openshiftapps.com`, `cert-api.access.redhat.com`, `api.access.redhat.com`, `infogw.api.openshift.com`, `console.redhat.com`, `observatorium-mst.api.openshift.com`, `observatorium.api.openshift.com`, `.amazonaws.com`, `ec2.amazonaws.com`, `events.<aws_region>.amazonaws.com`, `iam.amazonaws.com`, `route53.amazonaws.com`, `sts.amazonaws.com`, `mirror.openshift.com`, `api.openshift.com`, `api.pagerduty.com`, `events.pagerduty.com`, `api.deadmanssnitch.com`, `nosnch.in`, `http-inputs-osdsecuritylogs.splunkcloud.com`, `sftp.access.redhat.com` (recommended)"}
{"user_input": "We are investigating an incident which impacts multiple customers and need to update the Red Hat status page. How can we update the status page to have it visible for customers?", "reference": "If there is an ongoing incident that impacts customers you need to update the status page via InScope: https://inscope.corp.redhat.com/status-page and close the resolved issue once the incident is resolved."}
{"user_input": "I am testing a new webhook feature and need to pause Hive sync to be able to edit the cluster without it reconciling with Hive. How do I stop Hive from syncing everything including SyncSet and SelectorSyncSet?", "reference": "To pause Hive syncing and prevent reconciliation of SyncSets and SelectorSyncSets, follow these steps:\n\n1. **Find the ClusterDeployment:**\n   `oc get clusterdeployment -n <namespace>`\n\n2. **Add the pause annotation:**\n   `oc annotate clusterdeployment <clusterDeploymentName> -n <namespace> hive.openshift.io/syncset-pause=\"true\"`\n\nThis annotation will pause all Hive synchronization activities for the cluster, allowing you to make changes without interference from Hive's reconciliation process."}
{"user_input": "I am investigating HCP cluster issue related to ingress and need to run network verifier to make sure customer egress is not blocked. What command do I run to run network verifier for HCP?", "reference": "To perform baseline network verification for HCP cluster, use the OSD Network Verifier with pod-mode:\n\n`osdctl network verify-egress --pod-mode -C $CLUSTER_ID`\n\nThe `--pod-mode` flag is required for HCP clusters as it runs the verification from within pods on the hosted cluster, while `-C` specifies the cluster ID. This verifies that egress connectivity is working properly for the customer workloads."}
